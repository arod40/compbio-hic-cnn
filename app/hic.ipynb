{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIC similarity with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alero\\Anaconda3\\envs\\ht\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building positive image pairs\n",
      "Building image pairs for GSM1551552_HIC003 and GSM1551554_HIC005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 98.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building negative image pairs\n",
      "Building image pairs for GSM1551552_HIC003 and GSM1551569_HIC020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 92.33it/s] \n"
     ]
    }
   ],
   "source": [
    "from data_proc import HICPairsBioReplicatesDataset\n",
    "\n",
    "dataset = HICPairsBioReplicatesDataset(\n",
    "    \"../data/hic_dataset\",\n",
    "    [(\"GSM1551552_HIC003\", \"GSM1551554_HIC005\")],\n",
    "    [(\"GSM1551552_HIC003\", \"GSM1551569_HIC020\")],\n",
    "    None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling import SiameseNetwork, SiameseNetworkWithoutCNN\n",
    "\n",
    "# model = SiameseNetwork().to(device)\n",
    "model = SiameseNetworkWithoutCNN(40*40).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training params\n",
    "import torch\n",
    "\n",
    "batch_size = 400\n",
    "num_epochs = 10\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "checkpoint_dir = \"../checkpoints\"\n",
    "\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "train_data, test_data = torch.utils.data.random_split(dataset, [len(dataset) - 5000, 5000])\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_once(model, train_loader, criterion, optimizer):\n",
    "    print(\"Training model...\")\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input1, input2, label = batch[\"input1\"], batch[\"input2\"], batch[\"label\"]\n",
    "        output = model(input1.to(device), input2.to(device))\n",
    "        loss = criterion(output, label.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        print(f\"Batch: {i + 1}/{len(train_loader)}, Loss: {running_loss / (i + 1)}\", end=\"\\r\")\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def eval_once(model, test_data, criteria):\n",
    "    print(\"Evaluating model...\")\n",
    "    model.eval()\n",
    "\n",
    "    metrics = {name: 0.0 for name, _ in criteria}\n",
    "    with torch.no_grad():\n",
    "        for i,batch in enumerate(test_data):\n",
    "            input1, input2, label = batch[\"input1\"], batch[\"input2\"], batch[\"label\"]\n",
    "            output = model(input1.to(device), input2.to(device))\n",
    "\n",
    "            for name, criterion in criteria:\n",
    "                metric_value = criterion(output.squeeze(1).cpu(), label.squeeze(1).cpu())\n",
    "                metrics[name] += metric_value\n",
    "            print(f\"Batch: {i + 1}/{len(test_loader)}\", end=\"\\r\")\n",
    "\n",
    "    return {name: metric_value / len(test_data) for name, metric_value in metrics.items()}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training model...\n",
      "Evaluating model... 0.22944537575046223\n",
      "Train loss: 0.2294, Test loss: 0.0721 Test accuracy: 0.9744\n",
      "Epoch: 2\n",
      "Training model...\n",
      "Evaluating model... 0.06387675669975579\n",
      "Train loss: 0.0639, Test loss: 0.0581 Test accuracy: 0.9785\n",
      "Epoch: 3\n",
      "Training model...\n",
      "Evaluating model... 0.048279070667922495\n",
      "Train loss: 0.0483, Test loss: 0.0550 Test accuracy: 0.9775\n",
      "Epoch: 4\n",
      "Training model...\n",
      "Evaluating model... 0.037957294882896044\n",
      "Train loss: 0.0380, Test loss: 0.0491 Test accuracy: 0.9800\n",
      "Epoch: 5\n",
      "Training model...\n",
      "Evaluating model... 0.033697292829553284\n",
      "Train loss: 0.0337, Test loss: 0.0555 Test accuracy: 0.9754\n",
      "Epoch: 6\n",
      "Training model...\n",
      "Evaluating model... 0.028678430772076055\n",
      "Train loss: 0.0287, Test loss: 0.0536 Test accuracy: 0.9771\n",
      "Epoch: 7\n",
      "Training model...\n",
      "Evaluating model... 0.026637696944332373\n",
      "Train loss: 0.0266, Test loss: 0.0599 Test accuracy: 0.9754\n",
      "Epoch: 8\n",
      "Training model...\n",
      "Evaluating model... 0.020683017931878565\n",
      "Train loss: 0.0207, Test loss: 0.0593 Test accuracy: 0.9765\n",
      "Epoch: 9\n",
      "Training model...\n",
      "Evaluating model... 0.017636681340324382\n",
      "Train loss: 0.0176, Test loss: 0.0615 Test accuracy: 0.9756\n",
      "Epoch: 10\n",
      "Training model...\n",
      "Evaluating model... 0.015537620529842873\n",
      "Train loss: 0.0155, Test loss: 0.0769 Test accuracy: 0.9740\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Training loop\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"test_accuracy\": [],\n",
    "}\n",
    "eval_metrics = [(\"loss\", lambda x,y: criterion(x,y).item()), (\"accuracy\", lambda x, y: accuracy_score((x > 0.5).int(), y))]\n",
    "save_dir = Path(f\"../checkpoints/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "json.dump({\"batch_size\": batch_size, \"num_epochs\": num_epochs, \"criterion\": str(criterion), \"optimizer\": str(optimizer), \"model\": str(type(model))}, open(f\"{save_dir}/params.json\", \"w\"), indent=4)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    \n",
    "    history[\"train_loss\"].append(train_once(model, train_loader, criterion, optimizer))\n",
    "    epoch_val_metrics = eval_once(model, test_loader, eval_metrics)\n",
    "    for name, value in epoch_val_metrics.items():\n",
    "        history[f\"test_{name}\"].append(value)\n",
    "    \n",
    "    checkpoint_path = f\"{save_dir}/epoch_{epoch + 1}.pt\"\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "    print(f\"Train loss: {history['train_loss'][-1]:.4f}, Test loss: {epoch_val_metrics['loss']:.4f} Test accuracy: {epoch_val_metrics['accuracy']:.4f}\")\n",
    "torch.save(model.state_dict(), f\"{save_dir}/final.pt\")\n",
    "json.dump(history, open(f\"{save_dir}/history.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
